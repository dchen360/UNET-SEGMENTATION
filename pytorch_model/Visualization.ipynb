{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f662f211",
   "metadata": {},
   "source": [
    "# Segmentated Images\n",
    "Felipe Giuste 08-11-2020\n",
    "\n",
    "Modified by Danni Chen 02-26-2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cdae434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c85b3611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# ## PyTorch ##\n",
    "import torch\n",
    "from torch import nn #, optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "## TorchVision ##\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "## Utils ##\n",
    "from getClassificationStats import getStats\n",
    "from TrainTestSplit import ImageDataset\n",
    "\n",
    "## Plot ##\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "## Seed ##\n",
    "random_state = 1234\n",
    "np.random.seed(random_state)\n",
    "torch.manual_seed(random_state)\n",
    "torch.cuda.manual_seed(random_state)\n",
    "torch.cuda.manual_seed_all(random_state)\n",
    "\n",
    "## CUDNN ##\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "## Use best Device (CUDA vs CPU) ##\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "## Print Device Properties ##\n",
    "if device == torch.device('cuda'):\n",
    "    print( torch.cuda.get_device_properties( device ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d70d0c",
   "metadata": {},
   "source": [
    "# User Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506d4d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Name ##\n",
    "from datetime import datetime\n",
    "timeStamp = str(datetime.now())\n",
    "model_name = 'UNET_SEGMENTATION_COVID_LUNG ' + timeStamp\n",
    "## Import CAE class ##\n",
    "from utils.CAE_Segmentation_V18 import model\n",
    "\n",
    "## Model Weights path ##\n",
    "model_weights_path = 'model/%s_Final.pt'% model_name\n",
    "\n",
    "## Normalize before PCA ##\n",
    "normalize = False\n",
    "\n",
    "\n",
    "## Images per batch ##\n",
    "batch_size= 8\n",
    "\n",
    "## Image Directory ##\n",
    "image_directory = '/data/HeartTransplant/patches_2020-08-27_20X/' # 20X patches\n",
    "# '/data/HeartTransplant/patches_2020-08-14_10X/' # 10X patches\n",
    "# '/data/HeartTransplant/patches_2020-08-27_20X/' # 20X patches\n",
    "# '/data/HeartTransplant/patches_2020-08-05_40X/' # 40X patches\n",
    "\n",
    "# Metadata array:\n",
    "metadata_path= 'data/08-27-2020/metadata_2020-08-27_20X.csv' # 20X metadata\n",
    "# 'data/08-14-2020/metadata_2020-08-14_10X.csv' # 10X metadata\n",
    "# 'data/08-27-2020/metadata_2020-08-27_20X.csv' # 20X metadata\n",
    "# 'data/08-05-2020/metadata_2020-08-05_40X.csv' # 40X metadata\n",
    "\n",
    "## QC Variables ##\n",
    "focus_threshold = 3000 # 20X\n",
    "# 2500 # 10X\n",
    "# 3000 # 20X\n",
    "# 3000 # 40X\n",
    "grey_threshold  = 1000 # 20X\n",
    "# 1000 # 10X\n",
    "# 1000 # 20X\n",
    "# 1000 # 40X\n",
    "\n",
    "## Minimum Number of Tiles Passing QC ##\n",
    "nTilesThresh = 50\n",
    "\n",
    "## Metadata dataframe ##\n",
    "metadata = pd.read_csv( metadata_path, index_col=0, dtype= {'Subject':str} )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2423cccd",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f709a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## WSI to Test ##\n",
    "WSI_ID = 'S14-2657_2016-09-1419.43.29_he' # 1 Annotated, Scratched #4\n",
    "# 'CR1_018225_HE_2'\n",
    "# 'CR1_164876_HE_1' # Small\n",
    "# 'CR2_673196_HE_2' # Purple\n",
    "# 'S13-1900_2016-09-1417.42.38_he' # Pink\n",
    "# 'S14-2657_2016-09-1419.43.29_he' # 1 Annotated, Scratched #4\n",
    "\n",
    "## Testing Dataset: Metadata ##\n",
    "# tile_df = metadata[ metadata['ID'].str.startswith('C') ] # WSI name starts with 'C'\n",
    "# tile_df = metadata[ metadata['ID'].str.startswith('S') ] # WSI name starts with 'S'\n",
    "tile_df = metadata[ metadata['ID'] == WSI_ID ]\n",
    "\n",
    "## Testing Dataset: Define ##\n",
    "tile_dataset = ImageDataset( metadata=tile_df, root=image_directory, \n",
    "                             focus_threshold=focus_threshold, # QC\n",
    "                             grey_threshold=grey_threshold,   # QC\n",
    "                             nTilesThresh= nTilesThresh,      # QC\n",
    "                             augment=False # No Augmentation\n",
    "                           ) \n",
    "## Testing Dataset: Dataloader ##\n",
    "tile_loader = torch.utils.data.DataLoader(\n",
    "    tile_dataset, batch_size=batch_size, \n",
    "    pin_memory=True,\n",
    "    shuffle= True, # Set:False to match Tensorboard output\n",
    "    num_workers=40 )\n",
    "\n",
    "\n",
    "## Augmented Dataset: Define ##\n",
    "augmented_dataset = ImageDataset( metadata=tile_df, root=image_directory, \n",
    "                             focus_threshold=focus_threshold, # QC\n",
    "                             grey_threshold=grey_threshold,   # QC\n",
    "                             nTilesThresh= nTilesThresh,      # QC\n",
    "                             augment=True # Add Augmentation\n",
    "                           ) \n",
    "## Augmented Dataset: Dataloader ##\n",
    "augmented_loader = torch.utils.data.DataLoader(\n",
    "    tile_dataset, batch_size=batch_size, \n",
    "    pin_memory=True,\n",
    "    shuffle= True, # Set:False to match Tensorboard output\n",
    "    num_workers=40 )\n",
    "\n",
    "\n",
    "nPatches = len( augmented_dataset.metadata )\n",
    "print( 'Number of Patches (QC Passed): %s'% nPatches )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30169ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tiles: Total ##\n",
    "print( 'Tiles (Total): %s'% len(metadata) )\n",
    "## Tiles: Passed QC ##\n",
    "print( 'Tiles (Passed QC): %s'% len(tile_dataset.metadata) )\n",
    "\n",
    "## Non-Rejection Tiles (Passed QC) ##\n",
    "n_nonRejection = sum( tile_dataset.metadata['Label'] == 0 )\n",
    "print( 'Tiles Non-Rejection (Passed QC): %s'% n_nonRejection )\n",
    "\n",
    "## Rejection Tiles (Passed QC) ##\n",
    "n_Rejection = sum( tile_dataset.metadata['Label'] != 0 )\n",
    "print( 'Tiles Rejection (Passed QC): %s'% n_Rejection )\n",
    "\n",
    "print()\n",
    "## Non-Rejection WSI (Passed QC) ##\n",
    "no_rejection = tile_dataset.metadata['Label'] == 0\n",
    "n_nonRejection = len( tile_dataset.metadata[no_rejection]['ID'].unique() )\n",
    "print( 'WSI Non-Rejection (Passed QC): %s'% n_nonRejection )\n",
    "\n",
    "## Rejection WSI (Passed QC) ##\n",
    "rejection = tile_dataset.metadata['Label'] != 0\n",
    "n_Rejection = len( tile_dataset.metadata[rejection]['ID'].unique() )\n",
    "print( 'WSI Rejection (Passed QC): %s'% n_Rejection )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffbd2ee",
   "metadata": {},
   "source": [
    "# Model: Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47944e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model: Match Saved ##\n",
    "print(\"Total GPUs: %s\" %torch.cuda.device_count() )\n",
    "## Model saved as parallel ##\n",
    "if( list( torch.load(model_weights_path).keys() )[0].find('module.', 0, 7) != -1 ):\n",
    "    ## Parallel: adds 'model.' in front of model layers ##\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "## Load Trained Weights (Note: needs to be after DataParallel) ##\n",
    "model.load_state_dict( torch.load(model_weights_path), strict=True)\n",
    "# print( model.state_dict()['module.encoder.0.weight'][0] )\n",
    "\n",
    "## Freeze Model ##\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "    \n",
    "# Model: Send to device\n",
    "model.to(device);\n",
    "\n",
    "## Do not Train ##\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886a6125",
   "metadata": {},
   "source": [
    "# Normalize Encoded Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7735e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalize Channels (range 0-1) ##\n",
    "if(normalize):\n",
    "\n",
    "    # List of Channel means (list of lists) ##\n",
    "    batch_means = []\n",
    "    batch_maxs = []\n",
    "    batch_mins = []\n",
    "\n",
    "    ## Iterate across image batches ##\n",
    "    total_batches = len(tile_loader)\n",
    "    for batch_indx, item in enumerate(tile_loader):\n",
    "        print('Train Batch: %s / %s (%5.2f)     '% (batch_indx+1, total_batches, \n",
    "                                                    ((batch_indx+1)/total_batches)), end='\\r')\n",
    "        ## Assaign item (dict) values ##\n",
    "        img, label, _ = item.values()\n",
    "        ## Image to GPU ##\n",
    "        img = Variable(img).to(device)\n",
    "        ## Forward: encode->decode ##\n",
    "        encoded, _, _ = model(img)\n",
    "\n",
    "        ## Channel Means for batch ##\n",
    "        channel_means = [ torch.mean(encoded[:,channel]).item() for channel in range(encoded.shape[1]) ]\n",
    "        ## Append to batch_means list ##\n",
    "        batch_means.append( channel_means )\n",
    "\n",
    "        ## Channel Maxes for batch ##\n",
    "        channel_maxs = [ torch.max(encoded[:,channel]).item() for channel in range(encoded.shape[1]) ]\n",
    "        ## Append to batch_maxs list ##\n",
    "        batch_maxs.append( channel_maxs )\n",
    "\n",
    "        ## Channel Means for batch ##\n",
    "        channel_mins = [ torch.min(encoded[:,channel]).item() for channel in range(encoded.shape[1]) ]\n",
    "        ## Append to batch_mins list ##\n",
    "        batch_mins.append( channel_mins )\n",
    "\n",
    "    ## batch_means to Array (n_batches, n_channels) ##\n",
    "    batch_means = np.array(batch_means)\n",
    "    batch_maxs = np.array(batch_maxs)\n",
    "    batch_mins = np.array(batch_mins)\n",
    "\n",
    "    ## Channel Means ##\n",
    "    channel_means = np.mean( batch_means, axis=0 )\n",
    "    channel_maxs = np.max( batch_maxs, axis=0 )\n",
    "    channel_mins = np.min( batch_mins, axis=0 )\n",
    "\n",
    "    ## Subtract Mins from Maxes ##\n",
    "    channel_maxs = channel_maxs - channel_mins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a2ffd0",
   "metadata": {},
   "source": [
    "# Model Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ee2001",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Batches (iterator) ##\n",
    "batches = iter( tile_loader )\n",
    "\n",
    "## Divide encoded by tile maximum ##\n",
    "brighten = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125da471",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Batch y, encoded, reconstruction ###\n",
    "## Batch ##\n",
    "batch = next(batches)\n",
    "## Assaign item (dict) values ##\n",
    "img, y, ID = batch.values()\n",
    "## Image to GPU ##\n",
    "img = Variable(img).to(device)\n",
    "## Forward: encode->decode ##\n",
    "# encoded, _, reconstruction = model(img)\n",
    "model_output = model(img)\n",
    "encoded = model_output['encoded']\n",
    "reconstruction = model_output['reconstruction']\n",
    "\n",
    "## Normalize Channels (range 0-1) ##\n",
    "if(normalize):\n",
    "    ## Iterate across channels ##\n",
    "    for channel in range(encoded.shape[1]):\n",
    "        encoded[:, channel] = ( encoded[:, channel] - channel_mins[channel] ) / channel_maxs[channel]\n",
    "\n",
    "\n",
    "## Quilt rows and columns ##\n",
    "nCols= 2+encoded.shape[1]\n",
    "nRows=batch_size\n",
    "## Spacers ##\n",
    "wspace= 0.\n",
    "hspace= 0.25\n",
    "## Quilt width and height ##\n",
    "figsize_x = 20+(wspace*nCols)\n",
    "figsize_y = 20*(nRows/nCols)+(hspace*nRows)\n",
    "\n",
    "## Plot Tiles ##\n",
    "fig, ax = plt.subplots(nrows=nRows, ncols=nCols, figsize=(figsize_x, figsize_y), \n",
    "                       gridspec_kw = {'wspace':wspace, 'hspace':hspace}) \n",
    "\n",
    "## Don't change model ##\n",
    "model.eval()\n",
    "\n",
    "## While loops for control ##\n",
    "patch_index = 0\n",
    "i= 0\n",
    "while i < nRows:\n",
    "    ## Don't overflow ##\n",
    "    if( patch_index > batch['image'].shape[0] -1 ):\n",
    "        ## Next Batch ##\n",
    "        try:\n",
    "            batch = next(batches)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        ## Assaign item (dict) values ##\n",
    "        img, y, ID = batch.values()\n",
    "        ## Image to GPU ##\n",
    "        img = Variable(img).to(device)\n",
    "        ## Forward: encode->decode ##\n",
    "#         encoded, _, reconstruction = model(img)\n",
    "        model_output = model(img)\n",
    "        encoded = model_output['encoded']\n",
    "        reconstruction = model_output['reconstruction']\n",
    "        \n",
    "        ## Normalize Channels (range 0-1) ##\n",
    "        if(normalize):\n",
    "            ## Iterate across channels ##\n",
    "            for channel in range(encoded.shape[1]):\n",
    "                encoded[:, channel] = ( encoded[:, channel] - channel_mins[channel] ) / channel_maxs[channel]\n",
    "        \n",
    "        ## Reset Index ##\n",
    "        patch_index = 0\n",
    "\n",
    "        \n",
    "        \n",
    "    ## Original Image ##\n",
    "    image_original = img[patch_index].cpu().numpy()\n",
    "    image_original = np.moveaxis(image_original, 0, -1)\n",
    "\n",
    "    ## Encoded Image ##\n",
    "    image_encoded = encoded[patch_index].cpu().numpy()\n",
    "    image_encoded = np.moveaxis(image_encoded, 0, -1)\n",
    "    \n",
    "    ## Reconstructed Image ##\n",
    "    image_reconstruction = reconstruction[patch_index].cpu().numpy()\n",
    "    image_reconstruction = np.moveaxis(image_reconstruction, 0, -1)\n",
    "    \n",
    "    \n",
    "\n",
    "    ## Show Original ##\n",
    "    ax[i,0].imshow(image_original, vmin=0, vmax=1.)\n",
    "    ax[i,0].set_xticks([])\n",
    "    ax[i,0].set_yticks([])\n",
    "    ax[i,0].set_aspect('equal')\n",
    "    \n",
    "    ## Show Reconstruction ##\n",
    "    ax[i,1].imshow(image_reconstruction, vmin=0, vmax=1., cmap='Greys_r')\n",
    "    ax[i,1].set_xticks([])\n",
    "    ax[i,1].set_yticks([])\n",
    "    ax[i,1].set_aspect('equal')\n",
    "    \n",
    "    ## Iterate Across Encoded Channels ##\n",
    "    for channel in range(image_encoded.shape[2]):\n",
    "        ## Show Encoded ##\n",
    "        encoded_channel = image_encoded[:,:,channel]\n",
    "        if(brighten):\n",
    "            ax[i,channel+2].imshow(encoded_channel /np.max(encoded_channel), vmin=0, vmax=1., cmap='Greys_r') #\n",
    "        else:\n",
    "            ax[i,channel+2].imshow(encoded_channel, vmin=0, vmax=1., cmap='Greys_r') #\n",
    "        ax[i,channel+2].set_xticks([])\n",
    "        ax[i,channel+2].set_yticks([])\n",
    "        ax[i,channel+2].set_aspect('equal')\n",
    "        \n",
    "        ## Column Labels ##\n",
    "        if( patch_index == 0 ):\n",
    "            ax[i,channel+2].set_title('%s'% ('Encoded: %s'% channel), fontsize=12, color='blue')\n",
    "    \n",
    "    \n",
    "    ## Column Labels ##\n",
    "    if( patch_index == 0 ):\n",
    "        ax[i,0].set_title('%s'% ('Original'), fontsize=12, color='blue')\n",
    "        ax[i,1].set_title('%s'% ('Reconstruction'), fontsize=12, color='blue')\n",
    "    \n",
    "    \n",
    "    ## Image (Row) Label ##\n",
    "    y_label = str(ID[patch_index])\n",
    "    ax[i,0].set_ylabel('%s'% (y_label), \n",
    "                       fontsize=8,\n",
    "                       color='blue')\n",
    "\n",
    "    ## Next Tile ##\n",
    "    patch_index += 1\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1ce859",
   "metadata": {},
   "source": [
    "# Examine Tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb843ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo 'Skip'\n",
    "\n",
    "batch_index = 5\n",
    "\n",
    "## Encoded Image ##\n",
    "image_encoded = encoded[batch_index].cpu().numpy()\n",
    "image_encoded = np.moveaxis(image_encoded, 0, -1)\n",
    "\n",
    "## Original Image ##\n",
    "image_original = img[batch_index].cpu().numpy()\n",
    "image_original = np.moveaxis(image_original, 0, -1)\n",
    "\n",
    "np.max(image_encoded[:,:,2] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a15e6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo 'Skip'\n",
    "## Show Original ##\n",
    "plt.imshow(image_original, vmin=0, vmax=1.);\n",
    "plt.xticks([]);\n",
    "plt.yticks([]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0fe0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo 'Skip'\n",
    "## Show Encoded ##\n",
    "plt.imshow(image_encoded[:,:,:3], vmin=0, vmax=1.);\n",
    "plt.xticks([]);\n",
    "plt.yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295e61d5",
   "metadata": {},
   "source": [
    "# Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56023bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo 'Skip'\n",
    "plt.imshow( image_encoded[:,:,1], vmin=0, vmax=1., cmap='Greys_r' );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e60994",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo 'Skip'\n",
    "plt.hist( image_encoded[:,:,3].flatten(), bins=500 );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7f1dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo 'Skip'\n",
    "## C2 ##\n",
    "plt.imshow( image_encoded[:,:,2], vmin=0, vmax=1., cmap='Greys_r' );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7019dbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo 'Skip'\n",
    "import cv2\n",
    "\n",
    "## Average Mask (remove noise) ##\n",
    "image_blur = cv2.blur(image_encoded[:,:,2],(2,2))\n",
    "plt.imshow( image_blur, vmin=0, vmax=1., cmap='Greys_r' );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a00e6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo 'Skip'\n",
    "## ??? ##\n",
    "kernel = np.ones((2,2),np.uint8)\n",
    "\n",
    "image_morph = cv2.morphologyEx(image_encoded[:,:,2], cv2.MORPH_OPEN, kernel )\n",
    "image_morph = cv2.morphologyEx(image_morph, cv2.MORPH_CLOSE, kernel )\n",
    "plt.imshow( image_morph, vmin=0, vmax=1., cmap='Greys_r' );"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
